{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Fundamentals\n",
    "\n",
    "Contents:\n",
    "\n",
    "- Preparing Text for NLP\n",
    "    - Regular Expressions (regex)\n",
    "    - Tokenization\n",
    "    - Preprocessing Text\n",
    "        - lowercasing\n",
    "        - Removing Unwanted Tokens or Stopwords\n",
    "        - Lemmisation\n",
    "- Topic Identification\n",
    "    - Bag-of-Words\n",
    "    - Tf-idf\n",
    "- Named Entity Recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "\n",
    "import re\n",
    "\n",
    "from nltk import download\n",
    "#download('punkt')\n",
    "#download('wordnet')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import regexp_tokenize, TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Preparing Text for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex\n",
    "\n",
    "A _Regular Expression_ is a sequence of characters or a string containing a combination of normal (or regular, ordinary) characters and special metacharacters that define search patterns to find text or positions within a text. While normal characters have literal meaning and match themselves, metacharacters have special meaning and they resresent types of characters, such as `\\d` for digits or `\\w` for words, or ideas/repetitions.\n",
    "\n",
    ">Regular expressions use the backslash character ('\\') to indicate special forms or to allow special characters to be used without invoking their special meaning. This collides with Python’s usage of the same character for the same purpose in string literals. The solution is to use Python’s raw string notation for regular expression patterns; backslashes are not handled in any special way in a string literal prefixed with `'r'`. So `r\"\\n\"` is a two-character string containing `'\\'` and `'n'`, while `\"\\n\"` is a one-character string containing a newline. Usually patterns will be expressed in Python code using this raw string notation. [source](https://docs.python.org/3/library/re.html)\n",
    "\n",
    "Some of the special characters are shown in below tables.\n",
    "\n",
    "Metacharacter|Description\n",
    "---|:---\n",
    "`.` | Wildcard - Matches any single character except \\n.Matches any single character except `\\n`\n",
    "`^` | Matches the start of the string\n",
    "`$` | Matches the end of the string or just before the newline at the end of the string\n",
    "`\\|` | The OR (choice) operator matches either the expression before or the expression after the operator. For example, `abc\\|def` matches \"abc\" or \"def\".\n",
    "\n",
    "**Repetition Quantifiers:**\n",
    "\n",
    "Metacharacter|Description\n",
    "---|:---\n",
    "`*` | Matches the preceding element 0 or more times. `ab*` will match ‘a’, ‘ab’, or ‘a’ followed by any number of ‘b’s\n",
    "`+` | Matches the preceding element 1 or more times. `ab+` will match ‘a’ followed by any non-zero number of ‘b’s; it will not match just ‘a’\n",
    "`?` | Causes the resulting RE to match 0 or 1 repetitions of the preceding RE. `ab?` will match either ‘a’ or ‘ab’.\n",
    "`{m,n}` | Matches the preceding element at least m and not more than n times. For example, `a{3,5}` | Matches only \"aaa\", \"aaaa\", and \"aaaaa\"\n",
    "\n",
    "**Special Sequences:**\n",
    "\n",
    "Metacharacter|Description\n",
    "---|:---\n",
    "`\\w` | Matches alphanumeric characters, which means a-z, A-Z, and 0-9. It also matches the underscore, _.\n",
    "`\\d` | Matches digits, which means 0-9.\n",
    "`\\s` | Matches whitespace characters, which include the \\t, \\n, \\r, and space characters.\n",
    "`\\b` | Matches the boundary (or empty string) at the start and end of a word, that is, between \\w and \\W.\n",
    "`\\A` | Matches the beginning of a string (but not an internal line).\n",
    "`\\z` | Matches the end of a string (but not an internal line).\n",
    "\n",
    "**Ranges (Set of Characters):**\n",
    "\n",
    "Metacharacter|Description\n",
    "---|:---\n",
    "`[ ]` | Used to indicate a set of characters. Matches a single character that is contained within the brackets\n",
    "`[amk]` | Matches either a, m, or k. It does not match `amk`\n",
    "`[a-z]` | Matches any alphabet from `a` to `z`\n",
    "`[a\\-z]` | Matches a, -, or z. It matches `-` because `\\` escapes it\n",
    "`[a-cx-z]` | Matches a, b, c, x, y, z\n",
    "`[0-5][0-9]` | Matches all the two-digits numbers from 00 to 59\n",
    "`[a-]` | Matches a or -, because `-` is not being used to indicate a series of characters\n",
    "`[-a]` | As above, matches a or -\n",
    "`[a-z0-9]` | Matches characters from a to z and also from 0 to 9\n",
    "`[(+*)]` | Special characters become literal inside a set, so this matches (, +, *, and )\n",
    "`[^ab2]` | Adding ^ excludes any character in the set. Here, it matches characters that are not a, b, or 2\n",
    "\n",
    "**Groups:**\n",
    "\n",
    "Metacharacter|Description\n",
    "---|:---\n",
    "`( )` | Matches the expression inside the parentheses and groups it. Groups a series of pattern elements to a single element.\n",
    "`(? )` | Inside parentheses like this, ? acts as an extension notation. Its meaning depends on the character immediately to its right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['STRING: There are several sentence endings', ' One is Period', ' Another one is Question Mark', ' Other one is Exclamation Mark', ' [#Regex is great]']\n",
      "['STRING', 'There', 'One', 'Period', 'Another', 'Question', 'Mark', 'Other', 'Exclamation', 'Mark', 'Regex']\n",
      "['STRING:', 'There', 'are', 'several', 'sentence', 'endings.', 'One', 'is', 'Period.', 'Another', 'one', 'is', 'Question', 'Mark?', 'Other', 'one', 'is', 'Exclamation', 'Mark!', '[#Regex', 'is', 'great]']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "my_string = \"STRING: There are several sentence endings. One is Period. Another one is Question Mark? Other one is Exclamation Mark! [#Regex is great]\"\n",
    "\n",
    "# Split my_string on sentence endings and print the result\n",
    "sentence_endings = r\"[.?!]\"\n",
    "print(re.split(sentence_endings, my_string))\n",
    "\n",
    "# Find all capitalized words in my_string and print the result\n",
    "capitalized_words = r\"[A-Z]\\w+\"\n",
    "print(re.findall(capitalized_words, my_string))\n",
    "\n",
    "# Split my_string on spaces and print the result\n",
    "spaces = r\"\\s+\"\n",
    "print(re.split(spaces, my_string))\n",
    "\n",
    "# Find all digits in my_string and print the result\n",
    "digits = r\"\\d+\"\n",
    "print(re.findall(digits, my_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 70\n",
      "<re.Match object; span=(120, 137), match='[#Regex is great]'>\n",
      "<re.Match object; span=(0, 7), match='STRING:'>\n"
     ]
    }
   ],
   "source": [
    "# Search for the first occurrence of \"one\" in my_string\n",
    "match = re.search(\"one\", my_string)\n",
    "\n",
    "# Print the start and end indexes of match\n",
    "print(match.start(), match.end())\n",
    "\n",
    "# Write a regular expression to search for anything in square brackets\n",
    "pattern1 = r\"\\[.*\\]\"\n",
    "\n",
    "# Use re.search to find the first text in square brackets\n",
    "print(re.search(pattern1, my_string))\n",
    "\n",
    "# Find the script notation at the beginning and print it\n",
    "pattern2 = r\"[\\w\\s]+:\"\n",
    "print(re.match(pattern2, my_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Natural Language Toolkit (NLTK) is a platform for building Python programs to work with human language data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['STRING: There are several sentence endings.', 'One is Period.', 'Another one is Question Mark?', 'Other one is Exclamation Mark!', '[#Regex is great]']\n",
      "['Other', 'one', 'is', 'Exclamation', 'Mark', '!']\n",
      "{'endings', 'STRING', '[', '.', 'Mark', ':', 'is', 'sentence', 'great', '#', '!', 'are', 'several', 'Question', ']', 'Other', 'There', 'Regex', 'One', 'Period', '?', 'Another', 'Exclamation', 'one'}\n"
     ]
    }
   ],
   "source": [
    "#from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Split scene_one into sentences\n",
    "sentences = sent_tokenize(my_string)\n",
    "\n",
    "# Print the sentences\n",
    "print(sentences)\n",
    "\n",
    "# Use word_tokenize to tokenize the fourth sentence\n",
    "tokenized_sent = word_tokenize(sentences[3])\n",
    "\n",
    "# Print the tokenized sentence\n",
    "print(tokenized_sent)\n",
    "\n",
    "# Make a set of unique tokens in the entire scene\n",
    "unique_tokens = set(word_tokenize(my_string))\n",
    "\n",
    "# Print the unique tokens result\n",
    "print(unique_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 6), match='STRING'>\n",
      "['STRING', 'There', 'are', 'several', 'sentence', 'endings', '.', 'One', 'is', 'Period', '.', 'Another', 'one', 'is', 'Question', 'Mark', '?', 'Other', 'one', 'is', 'Exclamation', 'Mark', '!', '#Regex', 'is', 'great']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer pattern which retains sentence punctuation as seperate tokens. #... remains single token\n",
    "pattern3 = r\"(\\w+|#\\w+|\\?|!|\\.)\"\n",
    "\n",
    "print(re.match(pattern3, my_string))\n",
    "\n",
    "print(re.findall(pattern3, my_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['STRING',\n",
       " 'There',\n",
       " 'are',\n",
       " 'several',\n",
       " 'sentence',\n",
       " 'endings',\n",
       " '.',\n",
       " 'One',\n",
       " 'is',\n",
       " 'Period',\n",
       " '.',\n",
       " 'Another',\n",
       " 'one',\n",
       " 'is',\n",
       " 'Question',\n",
       " 'Mark',\n",
       " '?',\n",
       " 'Other',\n",
       " 'one',\n",
       " 'is',\n",
       " 'Exclamation',\n",
       " 'Mark',\n",
       " '!',\n",
       " '#Regex',\n",
       " 'is',\n",
       " 'great']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizer pattern which retains sentence punctuation as seperate tokens. #. remains single token\n",
    "pattern3 = r\"(\\w+|#\\w+|\\?|!|\\.)\"\n",
    "\n",
    "# Tokenize with the pattern\n",
    "regexp_tokenize(my_string, pattern3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#python', '#rstat']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Regex with NLTK tokenization\n",
    "\n",
    "tweets = [\"Data Science with #python and #rstat @DS now!\",\n",
    "         \"Natural Language Processing #NLP with NLTK\"]\n",
    "\n",
    "# Define a regex pattern to find hashtags: pattern1\n",
    "pattern4 = r\"#\\w+\"\n",
    "\n",
    "# Use the pattern on the first tweet in the tweets list\n",
    "regexp_tokenize(tweets[0], pattern4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#NLP']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write a pattern that matches both mentions and hashtags\n",
    "pattern5 = r\"([@#]\\w+)\"\n",
    "\n",
    "# Use the pattern on the last tweet in the tweets list\n",
    "regexp_tokenize(tweets[1], pattern5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Data', 'Science', 'with', '#python', 'and', '#rstat', '@DS', 'now', '!'], ['Natural', 'Language', 'Processing', '#NLP', 'with', 'NLTK']]\n"
     ]
    }
   ],
   "source": [
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "tknzr = TweetTokenizer()\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unicode ranges for emoji are:\n",
    "\n",
    "`('\\U0001F300'-'\\U0001F5FF')`, `('\\U0001F600-\\U0001F64F')`, `('\\U0001F680-\\U0001F6FF')`, and `('\\u2600'-\\u26FF-\\u2700-\\u27BF')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yemeğe', 'gidiyorum', '?', '🍕', 'İşin', 'var', 'mı', '?', '🚕']\n",
      "['Yemeğe', 'İşin']\n",
      "['🍕', '🚕']\n"
     ]
    }
   ],
   "source": [
    "# Non-ascii tokenization\n",
    "\n",
    "turkish_text = \"Yemeğe gidiyorum? 🍕 İşin var mı? 🚕\"\n",
    "\n",
    "# Tokenize and print all words in german_text\n",
    "all_words = word_tokenize(turkish_text)\n",
    "print(all_words)\n",
    "\n",
    "# Tokenize and print only capital words including thte non-ASCII characters\n",
    "capital_words = r\"[A-Zİ]\\w+\"\n",
    "print(regexp_tokenize(turkish_text, capital_words))\n",
    "\n",
    "# Tokenize and print only emoji\n",
    "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "print(regexp_tokenize(turkish_text, emoji))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], ['In', 'the', 'past', 'two', 'decades', 'there', 'has', 'been', 'a', 'significant', 'shift', 'in', 'naval', 'missions', 'toward', 'operations', 'other', 'than', 'war'], ['Maritime', 'security', 'operations', 'such', 'as', 'counter', 'piracy', 'maritime', 'interdiction', 'maritime', 'patrol', 'and', 'naval', 'escort', 'are', 'the', 'main', 'focus', 'of', 'most', 'fleets', 'today', 'however', 'the', 'vessels', 'that', 'are', 'currently', 'being', 'used', 'in', 'such', 'operations', 'were', 'mainly', 'built', 'for', 'other', 'purposes'], ['For', 'instance', 'in', 'August', '2009', 'the', 'North', 'Atlantic', 'Council', 'approved', 'Operation', 'Ocean', 'Shield', 'to', 'fight', 'piracy', 'in', 'the', 'Gulf', 'of', 'Aden'], ['Among', 'ships', 'that', 'were', 'assigned', 'in', 'the', 'rotations', 'of', 'this', 'NATO', 'mission', 'many', 'were', 'destroyers', 'and', 'frigates'], ['Although', 'those', 'warships', 'can', 'be', 'used', 'in', 'such', 'missions', 'how', 'reasonable', 'is', 'it', 'to', 'risk', 'a', 'destroyer', 'or', 'a', 'frigate', 'to', 'fight', 'with', 'terrorist', 'boats', 'or', 'pirates'], ['Capable', 'Warships', 'vs', 'Smaller', 'Combatants'], ['Many', 'surface', 'vessels', 'that', 'perform', 'maritime', 'security', 'operations', 'as', 'in', 'the', 'NATO', 'Task', 'Force', 'example', 'are', 'sophisticated', 'warships', 'capable', 'of', 'anti', 'surface', 'warfare', 'ASUW', 'anti', 'air', 'warfare', 'AAW', 'and', 'anti', 'submarine', 'warfare', 'ASW'], ['Although', 'these', 'sophisticated', 'multi', 'mission', 'capable', 'fleets', 'are', 'able', 'to', 'achieve', 'good', 'results', 'in', 'expeditionary', 'warfare', 'against', 'a', 'strong', 'enemy', '1', 'the', 'capabilities', 'of', 'those', 'ships', 'will', 'probably', 'be', 'used', 'in', 'less', 'than', '1', 'of', 'their', 'total', 'lifetime'], ['It', 'seems', 'a', 'sound', 'reason', 'to', 'build', 'capable', 'ships', 'in', 'case', 'of', 'a', 'conventional', 'war', 'and', 'one', 'can', 'claim', 'that', 'capable', 'ships', 'are', 'built', 'to', 'be', 'used', 'in', 'that', 'small', 'period', 'of', 'their', 'lifetime', 'nevertheless', 'navies', 'should', 'optimize', 'their', 'efforts', 'and', 'resources', 'in', 'some', 'way', 'to', 'find', 'a', 'better', 'mix', 'of', 'vessel', 'types', 'and', 'systems', 'that', 'constitute', 'the', 'vessels'], []]\n",
      "[0, 19, 39, 21, 17, 27, 5, 33, 38, 59, 0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('my_string_long.txt') as file:\n",
    "    my_string_long = file.read()\n",
    "\n",
    "# Split the script into lines\n",
    "lines = my_string_long.split('\\n')\n",
    "\n",
    "# Tokenize each line\n",
    "tokenized_lines = [regexp_tokenize(s, r\"\\w+\") for s in lines]\n",
    "print(tokenized_lines)\n",
    "\n",
    "# Make a frequency list of lengths\n",
    "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
    "print(line_num_words)\n",
    "\n",
    "# Plot a histogram of the line lengths\n",
    "plt.hist(line_num_words, bins=5)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Text\n",
    "\n",
    "You'll need to lower, remove stop words and non-alphabetic characters, lemmatize, and perform a new bag-of-words on your cleaned text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STRING: There are several sentence endings. One is Period. Another one is Question Mark? Other one is Exclamation Mark! [#Regex is great]\n",
      "['string', ':', 'there', 'are', 'several', 'sentence', 'endings', '.', 'one', 'is', 'period', '.', 'another', 'one', 'is', 'question', 'mark', '?', 'other', 'one', 'is', 'exclamation', 'mark', '!', '[', '#', 'regex', 'is', 'great', ']']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the article\n",
    "tokens = word_tokenize(my_string)\n",
    "\n",
    "# Convert the tokens into lowercase\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "print(my_string)\n",
    "print(lower_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the article\n",
    "tokens = word_tokenize(my_string_long)\n",
    "\n",
    "# Convert the tokens into lowercase\n",
    "lower_tokens = [t.lower() for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Unwanted Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stops = ['a', 'in', 'and', 'the', 'there', 'is', 'been', 'or', \n",
    "                 'has', 'are', 'one', 'is', 'other', 'this', 'these',\n",
    "                'those', 'that', 'of', 'an', 'it', 'to', 'for', 'be', 'on',\n",
    "                'such', 'were', 'they', 'their', 'as', 'has', 'vs', 'with']\n",
    "\n",
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in english_stops]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatisation\n",
    "\n",
    "Lemmatisation in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form. (In English, for example, run, runs, ran and running are forms of the same lexeme, with \"run\" as the lemma.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['past', 'two', 'decade', 'significant', 'shift', 'naval', 'mission', 'toward', 'operation', 'than', 'war', 'maritime', 'security', 'operation', 'maritime', 'interdiction', 'maritime', 'patrol', 'naval', 'escort', 'main', 'focus', 'most', 'fleet', 'today', 'however', 'vessel', 'currently', 'being', 'used', 'operation', 'mainly', 'built', 'purpose', 'instance', 'august', 'north', 'atlantic', 'council', 'approved', 'operation', 'ocean', 'shield', 'fight', 'piracy', 'gulf', 'aden', 'among', 'ship', 'assigned', 'rotation', 'nato', 'mission', 'many', 'destroyer', 'frigate', 'although', 'warship', 'can', 'used', 'mission', 'how', 'reasonable', 'risk', 'destroyer', 'frigate', 'fight', 'terrorist', 'boat', 'pirate', 'capable', 'warship', 'smaller', 'combatant', 'many', 'surface', 'vessel', 'perform', 'maritime', 'security', 'operation', 'nato', 'task', 'force', 'example', 'sophisticated', 'warship', 'capable', 'warfare', 'asuw', 'warfare', 'aaw', 'warfare', 'asw', 'although', 'sophisticated', 'capable', 'fleet', 'able', 'achieve', 'good', 'result', 'expeditionary', 'warfare', 'against', 'strong', 'enemy', 'capability', 'ship', 'will', 'probably', 'used', 'le', 'than', 'total', 'lifetime', 'seems', 'sound', 'reason', 'build', 'capable', 'ship', 'case', 'conventional', 'war', 'can', 'claim', 'capable', 'ship', 'built', 'used', 'small', 'period', 'lifetime', 'nevertheless', 'navy', 'should', 'optimize', 'effort', 'resource', 'some', 'way', 'find', 'better', 'mix', 'vessel', 'type', 'system', 'constitute', 'vessel']\n"
     ]
    }
   ],
   "source": [
    "# Import WordNetLemmatizer\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Topic Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Bag-of-Words\n",
    "\n",
    "Bag-of-words model can be used to represent the text as the bag of its words. It is mainly used as a tool of feature generation. One of the main properties of the bag-of-words model is that it completely ignores the order of the tokens in the document that is encoded, which is where the name bag-of-words comes from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag-of-words Using `Counter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 16), ('in', 13), ('the', 9), ('of', 9), ('.', 8), ('a', 7), ('to', 7), ('and', 6), ('that', 6), ('are', 5)]\n"
     ]
    }
   ],
   "source": [
    "# Import Counter\n",
    "# from collections import Counter\n",
    "\n",
    "# Tokenize the article\n",
    "tokens = word_tokenize(my_string_long)\n",
    "\n",
    "# Convert the tokens into lowercase\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "# Create a Counter with the lowercase tokens: bow_simple\n",
    "bow_simple = Counter(lower_tokens)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow_simple.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('operation', 5), ('capable', 5), ('maritime', 4), ('vessel', 4), ('used', 4), ('ship', 4), ('warfare', 4), ('mission', 3), ('warship', 3), ('naval', 2)]\n"
     ]
    }
   ],
   "source": [
    "# Create the bag-of-words: bow\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag-of-words Using `gensim`\n",
    "\n",
    "Word vectors are multi-dimensional mathematical representations of words created using deep learning methods. They give us insight into relationships between words in a corpus. We can create and query a corpus with `gensim` module.\n",
    "\n",
    "Gensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora.\n",
    "\n",
    "Check [gensim](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/gensim%20Quick%20Start.ipynb) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = \"raw-corpus/\"\n",
    "\n",
    "files = os.listdir(path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# Import text files to generate raw corpus\n",
    "for file in files:\n",
    "    if file.endswith(\".txt\"):\n",
    "        with open(path + file, 'r') as f:\n",
    "            raw_corpus.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing text (simple)\n",
    "\n",
    "# Lowercase each document, split it by white space and filter out stopwords\n",
    "texts = [[word for word in document.lower().split() if word not in english_stops]\n",
    "         for document in raw_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding, we want to associate each word in the corpus with a unique integer ID. We can do this using the `gensim.corpora.Dictionary` class. This dictionary defines the vocabulary of all words that our processing knows about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(220 unique tokens: ['analysis', 'cleansing,', 'conclusions', 'data', 'decision-making.']...)\n"
     ]
    }
   ],
   "source": [
    "# Import Dictionary\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Create a Dictionary from the articles\n",
    "dictionary = Dictionary(texts)\n",
    "\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our processed corpus has 223 unique words in it, which means that each document will be represented by a 223-dimensional vector under the bag-of-words model. We can use the dictionary to turn tokenized documents into these 223-dimensional vectors. We can see what these IDs correspond to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'analysis': 0, 'cleansing,': 1, 'conclusions': 2, 'data': 3, 'decision-making.': 4, 'discovering': 5, 'goal': 6, 'information,': 7, 'informing': 8, 'inspecting,': 9, 'modeling': 10, 'process': 11, 'supporting': 12, 'transforming': 13, 'useful': 14, '0': 15, '1': 16, '1,': 17, 'between': 18, 'certainty.': 19, 'events': 20, 'glossary': 21, 'impossibility': 22, 'indicates': 23, 'likelihood': 24, 'measure': 25, 'number': 26, 'occur.': 27, 'probability': 28, 'quantifies': 29, 'quantifying': 30, 'see': 31, 'sparsly': 32, 'speaking,': 33, 'statistics.': 34, 'where,': 35, 'will': 36, '\"a': 37, '\"can': 38, '\"computing': 39, '\"training': 40, '(as': 41, '1959': 42, '====': 43, 'across': 44, 'alan': 45, 'algorithm': 46, 'algorithms': 47, 'also': 48, 'analytics.': 49, 'application': 50, 'applications,': 51, 'arthur': 52, 'artificial': 53, 'at': 54, 'based': 55, 'being': 56, 'build': 57, 'business': 58, 'by': 59, 'can': 60, 'characteristics': 61, 'class': 62, 'closely': 63, 'cognitive': 64, 'coined': 65, 'computational': 66, 'computer': 67, 'computers.': 68, 'concerned': 69, 'constructing': 70, 'could': 71, 'data\",': 72, 'data,': 73, 'decisions': 74, 'defining': 75, 'definition': 76, 'delivers': 77, 'develop': 78, 'do': 79, 'do?\".': 80, 'domains': 81, 'e': 82, 'e.\"': 83, 'effectively': 84, 'email': 85, 'entities)': 86, 'experience': 87, 'explicit': 88, 'explicitly': 89, 'exploratory': 90, 'exposed.': 91, 'field': 92, 'field:': 93, 'filtering,': 94, 'focuses': 95, 'follows': 96, 'formal': 97, 'from': 98, 'fundamentally': 99, 'his': 100, 'if': 101, 'implications': 102, 'improves': 103, 'infeasible': 104, 'inference': 105, 'instead.': 106, 'instructions': 107, 'instructions,': 108, 'intelligence\",': 109, 'intelligence.': 110, 'its': 111, 'known': 112, 'learn': 113, 'learning': 114, 'learning,': 115, 'learning.': 116, 'm.': 117, 'machine': 118, 'machinery': 119, 'machines': 120, 'make': 121, 'making': 122, 'mathematical': 123, 'measured': 124, 'methods,': 125, 'mining': 126, 'mitchell': 127, 'model': 128, 'models': 129, 'more': 130, 'name': 131, 'offers': 132, 'operational': 133, 'optimization': 134, 'order': 135, 'overview': 136, 'p': 137, 'p,': 138, 'paper': 139, 'patterns': 140, 'perform': 141, 'performance': 142, 'performing': 143, 'possessed': 144, 'predictions': 145, 'predictive': 146, 'problems,': 147, 'program': 148, 'programmed': 149, 'proposal': 150, 'provided': 151, 'question': 152, 'quoted,': 153, 'rather': 154, 'referred': 155, 'related': 156, 'relying': 157, 'replaced': 158, 'respect': 159, 'said': 160, 'sample': 161, 'samuel.': 162, 'scientific': 163, 'seen': 164, 'some': 165, 'specific': 166, 'statistical': 167, 'statistics,': 168, 'studied': 169, 'study': 170, 'subset': 171, 'systems': 172, 't': 173, 't,': 174, 'task': 175, 'task.': 176, 'tasks': 177, 'terms.': 178, 'than': 179, 'theory': 180, 'think?\"': 181, 'thinking': 182, 'through': 183, 'tom': 184, \"turing's\": 185, 'unsupervised': 186, 'use': 187, 'used': 188, 'using': 189, 'variety': 190, 'various': 191, 'vision,': 192, 'was': 193, 'we': 194, 'what': 195, 'where': 196, 'which': 197, 'wide': 198, 'widely': 199, 'within': 200, 'without': 201, 'analysis,': 202, 'applying': 203, 'begin': 204, 'branch': 205, 'collection,': 206, 'conventional': 207, 'industrial,': 208, 'interpretation': 209, 'mathematics': 210, 'organization,': 211, 'population': 212, 'presentation.': 213, 'problem,': 214, 'scientific,': 215, 'social': 216, 'statistics': 217, 'studied.': 218, 'working': 219}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 3\n",
      "word:  data\n"
     ]
    }
   ],
   "source": [
    "# Select the id for \"data\"\n",
    "data_id = dictionary.token2id.get(\"data\")\n",
    "\n",
    "print(\"id:\", data_id)\n",
    "\n",
    "# Use data_id with the dictionary to print the word\n",
    "print(\"word: \", dictionary.get(data_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create the bag-of-word representation for a document using the doc2bow method of the dictionary, which returns a sparse representation of the word counts. The first entry in each tuple corresponds to the ID of the token in the dictionary, the second corresponds to the count of this token. Now, we can convert our entire original corpus to a list of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 1), (128, 1), (167, 2), (202, 1), (203, 1), (204, 1), (205, 1), (206, 1), (207, 1), (208, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Create a MmCorpus\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Print the first 10 word ids with their frequency counts from the fourth document\n",
    "print(bow_corpus[3][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of documents (texts)\n",
    "len(bow_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even vectorize a phrase, \"Data is everywhere\", which was not in our original corpus. Note that the word \"everywhere\" did not occur in the original corpus and so it was not included in the vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 1)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text = \"Data is everywhere\"\n",
    "\n",
    "# Create the bag-of-word representation for the new_text using the doc2bow method of the dictionary\n",
    "new_vec = dictionary.doc2bow(new_text.lower().split())\n",
    "\n",
    "# Print the new_vec\n",
    "new_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(167, 2),\n",
       " (217, 2),\n",
       " (3, 1),\n",
       " (128, 1),\n",
       " (202, 1),\n",
       " (203, 1),\n",
       " (204, 1),\n",
       " (205, 1),\n",
       " (206, 1),\n",
       " (207, 1),\n",
       " (208, 1),\n",
       " (209, 1),\n",
       " (210, 1),\n",
       " (211, 1),\n",
       " (212, 1),\n",
       " (213, 1),\n",
       " (214, 1),\n",
       " (215, 1),\n",
       " (216, 1),\n",
       " (218, 1),\n",
       " (219, 1)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the fourth document\n",
    "doc = bow_corpus[3]\n",
    "\n",
    "# Sort the doc for frequency\n",
    "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "bow_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistical 2\n",
      "statistics 2\n",
      "data 1\n",
      "model 1\n",
      "analysis, 1\n"
     ]
    }
   ],
   "source": [
    "# Print the top 5 words of the document alongside the count\n",
    "for word_id, word_count in bow_doc[:5]:\n",
    "    print(dictionary.get(word_id), word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine 11\n",
      "learning 8\n",
      "data 5\n",
      "algorithms 4\n",
      "probability 3\n"
     ]
    }
   ],
   "source": [
    "# Create the defaultdict\n",
    "total_word_count = defaultdict(int)\n",
    "for word_id, word_count in chain.from_iterable(bow_corpus): #itertools.chain\n",
    "    total_word_count[word_id] += word_count\n",
    "\n",
    "# Create a sorted list from the defaultdict\n",
    "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) \n",
    "\n",
    "# Print the top 5 words across all documents alongside the count\n",
    "for word_id, word_count in sorted_word_count[:5]:\n",
    "    print(dictionary.get(word_id), word_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
