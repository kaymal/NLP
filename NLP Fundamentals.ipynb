{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Fundamentals\n",
    "\n",
    "Contents:\n",
    "\n",
    "- Preparing Text for NLP\n",
    "    - Regular Expressions (regex)\n",
    "    - Tokenization\n",
    "    - Preprocessing Text\n",
    "        - lowercasing\n",
    "        - Removing Unwanted Tokens or Stopwords\n",
    "        - Lemmisation\n",
    "- Topic Identification\n",
    "    - Bag-of-Words\n",
    "        - Counter\n",
    "        - gensim\n",
    "    - Tf-idf\n",
    "- Named Entity Recognition\n",
    "    - NER with NLTK\n",
    "    - NER with SpaCy\n",
    "    - NER with polyglot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('averaged_perceptron_tagger') # NER\n",
    "# nltk.download('maxent_ne_chunker') # NER\n",
    "# nltk.download('words') # NER\n",
    "# nltk.download('tagsets') #NER to see the tags available\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import regexp_tokenize, TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "import spacy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Preparing Text for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex\n",
    "\n",
    "A _Regular Expression_ is a sequence of characters or a string containing a combination of normal (or regular, ordinary) characters and special metacharacters that define search patterns to find text or positions within a text. While normal characters have literal meaning and match themselves, metacharacters have special meaning and they resresent types of characters, such as `\\d` for digits or `\\w` for words, or ideas/repetitions.\n",
    "\n",
    ">Regular expressions use the backslash character ('\\') to indicate special forms or to allow special characters to be used without invoking their special meaning. This collides with Python’s usage of the same character for the same purpose in string literals. The solution is to use Python’s raw string notation for regular expression patterns; backslashes are not handled in any special way in a string literal prefixed with `'r'`. So `r\"\\n\"` is a two-character string containing `'\\'` and `'n'`, while `\"\\n\"` is a one-character string containing a newline. Usually patterns will be expressed in Python code using this raw string notation. [source](https://docs.python.org/3/library/re.html)\n",
    "\n",
    "Some of the special characters are shown in below tables.\n",
    "\n",
    "Metacharacter|Description\n",
    "---|:---\n",
    "`.` | Wildcard - Matches any single character except \\n.Matches any single character except `\\n`\n",
    "`^` | Matches the start of the string\n",
    "`$` | Matches the end of the string or just before the newline at the end of the string\n",
    "`\\|` | The OR (choice) operator matches either the expression before or the expression after the operator. For example, `abc\\|def` matches \"abc\" or \"def\".\n",
    "\n",
    "**Repetition Quantifiers:**\n",
    "\n",
    "Metacharacter|Description\n",
    "---|:---\n",
    "`*` | Matches the preceding element 0 or more times. `ab*` will match ‘a’, ‘ab’, or ‘a’ followed by any number of ‘b’s\n",
    "`+` | Matches the preceding element 1 or more times. `ab+` will match ‘a’ followed by any non-zero number of ‘b’s; it will not match just ‘a’\n",
    "`?` | Causes the resulting RE to match 0 or 1 repetitions of the preceding RE. `ab?` will match either ‘a’ or ‘ab’.\n",
    "`{m,n}` | Matches the preceding element at least m and not more than n times. For example, `a{3,5}` | Matches only \"aaa\", \"aaaa\", and \"aaaaa\"\n",
    "\n",
    "**Special Sequences:**\n",
    "\n",
    "Metacharacter|Description\n",
    "---|:---\n",
    "`\\w` | Matches alphanumeric characters, which means a-z, A-Z, and 0-9. It also matches the underscore, _.\n",
    "`\\d` | Matches digits, which means 0-9.\n",
    "`\\s` | Matches whitespace characters, which include the \\t, \\n, \\r, and space characters.\n",
    "`\\b` | Matches the boundary (or empty string) at the start and end of a word, that is, between \\w and \\W.\n",
    "`\\A` | Matches the beginning of a string (but not an internal line).\n",
    "`\\z` | Matches the end of a string (but not an internal line).\n",
    "\n",
    "**Ranges (Set of Characters):**\n",
    "\n",
    "Metacharacter|Description\n",
    "---|:---\n",
    "`[ ]` | Used to indicate a set of characters. Matches a single character that is contained within the brackets\n",
    "`[amk]` | Matches either a, m, or k. It does not match `amk`\n",
    "`[a-z]` | Matches any alphabet from `a` to `z`\n",
    "`[a\\-z]` | Matches a, -, or z. It matches `-` because `\\` escapes it\n",
    "`[a-cx-z]` | Matches a, b, c, x, y, z\n",
    "`[0-5][0-9]` | Matches all the two-digits numbers from 00 to 59\n",
    "`[a-]` | Matches a or -, because `-` is not being used to indicate a series of characters\n",
    "`[-a]` | As above, matches a or -\n",
    "`[a-z0-9]` | Matches characters from a to z and also from 0 to 9\n",
    "`[(+*)]` | Special characters become literal inside a set, so this matches (, +, *, and )\n",
    "`[^ab2]` | Adding ^ excludes any character in the set. Here, it matches characters that are not a, b, or 2\n",
    "\n",
    "**Groups:**\n",
    "\n",
    "Metacharacter|Description\n",
    "---|:---\n",
    "`( )` | Matches the expression inside the parentheses and groups it. Groups a series of pattern elements to a single element.\n",
    "`(? )` | Inside parentheses like this, ? acts as an extension notation. Its meaning depends on the character immediately to its right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['STRING: There are several sentence endings', ' One is Period', ' Another one is Question Mark', ' Other one is Exclamation Mark', ' [#Regex is great]']\n",
      "['STRING', 'There', 'One', 'Period', 'Another', 'Question', 'Mark', 'Other', 'Exclamation', 'Mark', 'Regex']\n",
      "['STRING:', 'There', 'are', 'several', 'sentence', 'endings.', 'One', 'is', 'Period.', 'Another', 'one', 'is', 'Question', 'Mark?', 'Other', 'one', 'is', 'Exclamation', 'Mark!', '[#Regex', 'is', 'great]']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "my_string = \"STRING: There are several sentence endings. One is Period. Another one is Question Mark? Other one is Exclamation Mark! [#Regex is great]\"\n",
    "\n",
    "# Split my_string on sentence endings and print the result\n",
    "sentence_endings = r\"[.?!]\"\n",
    "print(re.split(sentence_endings, my_string))\n",
    "\n",
    "# Find all capitalized words in my_string and print the result\n",
    "capitalized_words = r\"[A-Z]\\w+\"\n",
    "print(re.findall(capitalized_words, my_string))\n",
    "\n",
    "# Split my_string on spaces and print the result\n",
    "spaces = r\"\\s+\"\n",
    "print(re.split(spaces, my_string))\n",
    "\n",
    "# Find all digits in my_string and print the result\n",
    "digits = r\"\\d+\"\n",
    "print(re.findall(digits, my_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 70\n",
      "<re.Match object; span=(120, 137), match='[#Regex is great]'>\n",
      "<re.Match object; span=(0, 7), match='STRING:'>\n"
     ]
    }
   ],
   "source": [
    "# Search for the first occurrence of \"one\" in my_string\n",
    "match = re.search(\"one\", my_string)\n",
    "\n",
    "# Print the start and end indexes of match\n",
    "print(match.start(), match.end())\n",
    "\n",
    "# Write a regular expression to search for anything in square brackets\n",
    "pattern1 = r\"\\[.*\\]\"\n",
    "\n",
    "# Use re.search to find the first text in square brackets\n",
    "print(re.search(pattern1, my_string))\n",
    "\n",
    "# Find the script notation at the beginning and print it\n",
    "pattern2 = r\"[\\w\\s]+:\"\n",
    "print(re.match(pattern2, my_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Natural Language Toolkit (NLTK) is a platform for building Python programs to work with human language data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['STRING: There are several sentence endings.', 'One is Period.', 'Another one is Question Mark?', 'Other one is Exclamation Mark!', '[#Regex is great]']\n",
      "['Other', 'one', 'is', 'Exclamation', 'Mark', '!']\n",
      "{'[', '#', 'one', ']', '.', 'STRING', 'great', 'There', ':', 'are', 'is', 'several', 'sentence', 'endings', '!', 'Period', '?', 'Other', 'Question', 'Mark', 'One', 'Exclamation', 'Regex', 'Another'}\n"
     ]
    }
   ],
   "source": [
    "#from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Split scene_one into sentences\n",
    "sentences = sent_tokenize(my_string)\n",
    "\n",
    "# Print the sentences\n",
    "print(sentences)\n",
    "\n",
    "# Use word_tokenize to tokenize the fourth sentence\n",
    "tokenized_sent = word_tokenize(sentences[3])\n",
    "\n",
    "# Print the tokenized sentence\n",
    "print(tokenized_sent)\n",
    "\n",
    "# Make a set of unique tokens in the entire scene\n",
    "unique_tokens = set(word_tokenize(my_string))\n",
    "\n",
    "# Print the unique tokens result\n",
    "print(unique_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 6), match='STRING'>\n",
      "['STRING', 'There', 'are', 'several', 'sentence', 'endings', '.', 'One', 'is', 'Period', '.', 'Another', 'one', 'is', 'Question', 'Mark', '?', 'Other', 'one', 'is', 'Exclamation', 'Mark', '!', '#Regex', 'is', 'great']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer pattern which retains sentence punctuation as seperate tokens. #... remains single token\n",
    "pattern3 = r\"(\\w+|#\\w+|\\?|!|\\.)\"\n",
    "\n",
    "print(re.match(pattern3, my_string))\n",
    "\n",
    "print(re.findall(pattern3, my_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['STRING',\n",
       " 'There',\n",
       " 'are',\n",
       " 'several',\n",
       " 'sentence',\n",
       " 'endings',\n",
       " '.',\n",
       " 'One',\n",
       " 'is',\n",
       " 'Period',\n",
       " '.',\n",
       " 'Another',\n",
       " 'one',\n",
       " 'is',\n",
       " 'Question',\n",
       " 'Mark',\n",
       " '?',\n",
       " 'Other',\n",
       " 'one',\n",
       " 'is',\n",
       " 'Exclamation',\n",
       " 'Mark',\n",
       " '!',\n",
       " '#Regex',\n",
       " 'is',\n",
       " 'great']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizer pattern which retains sentence punctuation as seperate tokens. #. remains single token\n",
    "pattern3 = r\"(\\w+|#\\w+|\\?|!|\\.)\"\n",
    "\n",
    "# Tokenize with the pattern\n",
    "regexp_tokenize(my_string, pattern3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#python', '#rstat']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Regex with NLTK tokenization\n",
    "\n",
    "tweets = [\"Data Science with #python and #rstat @DS now!\",\n",
    "         \"Natural Language Processing #NLP with NLTK\"]\n",
    "\n",
    "# Define a regex pattern to find hashtags\n",
    "pattern4 = r\"#\\w+\"\n",
    "\n",
    "# Use the pattern on the first tweet in the tweets list\n",
    "regexp_tokenize(tweets[0], pattern4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#NLP']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write a pattern that matches both mentions and hashtags\n",
    "pattern5 = r\"([@#]\\w+)\"\n",
    "\n",
    "# Use the pattern on the last tweet in the tweets list\n",
    "regexp_tokenize(tweets[1], pattern5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Data', 'Science', 'with', '#python', 'and', '#rstat', '@DS', 'now', '!'], ['Natural', 'Language', 'Processing', '#NLP', 'with', 'NLTK']]\n"
     ]
    }
   ],
   "source": [
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "tknzr = TweetTokenizer()\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unicode ranges for emoji are:\n",
    "\n",
    "`('\\U0001F300'-'\\U0001F5FF')`, `('\\U0001F600-\\U0001F64F')`, `('\\U0001F680-\\U0001F6FF')`, and `('\\u2600'-\\u26FF-\\u2700-\\u27BF')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yemeğe', 'gidiyorum', '?', '🍕', 'İşin', 'var', 'mı', '?', '🚕']\n",
      "['Yemeğe', 'İşin']\n",
      "['🍕', '🚕']\n"
     ]
    }
   ],
   "source": [
    "# Non-ascii tokenization\n",
    "\n",
    "turkish_text = \"Yemeğe gidiyorum? 🍕 İşin var mı? 🚕\"\n",
    "\n",
    "# Tokenize and print all words in german_text\n",
    "all_words = word_tokenize(turkish_text)\n",
    "print(all_words)\n",
    "\n",
    "# Tokenize and print only capital words including the non-ASCII characters\n",
    "capital_words = r\"[A-Zİ]\\w+\"\n",
    "print(regexp_tokenize(turkish_text, capital_words))\n",
    "\n",
    "# Tokenize and print only emoji\n",
    "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "print(regexp_tokenize(turkish_text, emoji))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], ['In', 'the', 'past', 'two', 'decades', 'there', 'has', 'been', 'a', 'significant', 'shift', 'in', 'naval', 'missions', 'toward', 'operations', 'other', 'than', 'war'], ['Maritime', 'security', 'operations', 'such', 'as', 'counter', 'piracy', 'maritime', 'interdiction', 'maritime', 'patrol', 'and', 'naval', 'escort', 'are', 'the', 'main', 'focus', 'of', 'most', 'fleets', 'today', 'however', 'the', 'vessels', 'that', 'are', 'currently', 'being', 'used', 'in', 'such', 'operations', 'were', 'mainly', 'built', 'for', 'other', 'purposes'], ['For', 'instance', 'in', 'August', '2009', 'the', 'North', 'Atlantic', 'Council', 'approved', 'Operation', 'Ocean', 'Shield', 'to', 'fight', 'piracy', 'in', 'the', 'Gulf', 'of', 'Aden'], ['Among', 'ships', 'that', 'were', 'assigned', 'in', 'the', 'rotations', 'of', 'this', 'NATO', 'mission', 'many', 'were', 'destroyers', 'and', 'frigates'], ['Although', 'those', 'warships', 'can', 'be', 'used', 'in', 'such', 'missions', 'how', 'reasonable', 'is', 'it', 'to', 'risk', 'a', 'destroyer', 'or', 'a', 'frigate', 'to', 'fight', 'with', 'terrorist', 'boats', 'or', 'pirates'], ['Capable', 'Warships', 'vs', 'Smaller', 'Combatants'], ['Many', 'surface', 'vessels', 'that', 'perform', 'maritime', 'security', 'operations', 'as', 'in', 'the', 'NATO', 'Task', 'Force', 'example', 'are', 'sophisticated', 'warships', 'capable', 'of', 'anti', 'surface', 'warfare', 'ASUW', 'anti', 'air', 'warfare', 'AAW', 'and', 'anti', 'submarine', 'warfare', 'ASW'], ['Although', 'these', 'sophisticated', 'multi', 'mission', 'capable', 'fleets', 'are', 'able', 'to', 'achieve', 'good', 'results', 'in', 'expeditionary', 'warfare', 'against', 'a', 'strong', 'enemy', '1', 'the', 'capabilities', 'of', 'those', 'ships', 'will', 'probably', 'be', 'used', 'in', 'less', 'than', '1', 'of', 'their', 'total', 'lifetime'], ['It', 'seems', 'a', 'sound', 'reason', 'to', 'build', 'capable', 'ships', 'in', 'case', 'of', 'a', 'conventional', 'war', 'and', 'one', 'can', 'claim', 'that', 'capable', 'ships', 'are', 'built', 'to', 'be', 'used', 'in', 'that', 'small', 'period', 'of', 'their', 'lifetime', 'nevertheless', 'navies', 'should', 'optimize', 'their', 'efforts', 'and', 'resources', 'in', 'some', 'way', 'to', 'find', 'a', 'better', 'mix', 'of', 'vessel', 'types', 'and', 'systems', 'that', 'constitute', 'the', 'vessels'], []]\n"
     ]
    }
   ],
   "source": [
    "with open('my_string_long.txt') as file:\n",
    "    my_string_long = file.read()\n",
    "\n",
    "# Split the script into lines\n",
    "lines = my_string_long.split('\\n')\n",
    "\n",
    "# Tokenize each line\n",
    "tokenized_lines = [regexp_tokenize(s, r\"\\w+\") for s in lines]\n",
    "print(tokenized_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 19, 39, 21, 17, 27, 5, 33, 38, 59, 0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make a frequency list of lengths\n",
    "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
    "print(line_num_words)\n",
    "\n",
    "# Plot a histogram of the line lengths\n",
    "plt.hist(line_num_words, bins=5)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Text\n",
    "\n",
    "You'll need to lower, remove stop words and non-alphabetic characters, lemmatize, and perform a new bag-of-words on your cleaned text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STRING: There are several sentence endings. One is Period. Another one is Question Mark? Other one is Exclamation Mark! [#Regex is great]\n",
      "['string', ':', 'there', 'are', 'several', 'sentence', 'endings', '.', 'one', 'is', 'period', '.', 'another', 'one', 'is', 'question', 'mark', '?', 'other', 'one', 'is', 'exclamation', 'mark', '!', '[', '#', 'regex', 'is', 'great', ']']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the article\n",
    "tokens = word_tokenize(my_string)\n",
    "\n",
    "# Convert the tokens into lowercase\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "print(my_string)\n",
    "print(lower_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the article\n",
    "tokens = word_tokenize(my_string_long)\n",
    "\n",
    "# Convert the tokens into lowercase\n",
    "lower_tokens = [t.lower() for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Unwanted Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some English stops (not all of them!)\n",
    "english_stops = ['a', 'in', 'and', 'the', 'there', 'is', 'been', 'or', \n",
    "                 'has', 'are', 'one', 'is', 'other', 'this', 'these',\n",
    "                'those', 'that', 'of', 'an', 'it', 'to', 'for', 'be', 'on',\n",
    "                'such', 'were', 'they', 'their', 'as', 'has', 'vs', 'with',\n",
    "                'most', 'although', 'can', 'than']\n",
    "\n",
    "# Retain alphabetic words\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in english_stops]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatisation\n",
    "\n",
    "Lemmatisation in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form. (In English, for example, run, runs, ran and running are forms of the same lexeme, with \"run\" as the lemma.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['past', 'two', 'decade', 'significant', 'shift', 'naval', 'mission', 'toward', 'operation', 'war', 'maritime', 'security', 'operation', 'maritime', 'interdiction', 'maritime', 'patrol', 'naval', 'escort', 'main', 'focus', 'fleet', 'today', 'however', 'vessel', 'currently', 'being', 'used', 'operation', 'mainly', 'built', 'purpose', 'instance', 'august', 'north', 'atlantic', 'council', 'approved', 'operation', 'ocean', 'shield', 'fight', 'piracy', 'gulf', 'aden', 'among', 'ship', 'assigned', 'rotation', 'nato', 'mission', 'many', 'destroyer', 'frigate', 'warship', 'used', 'mission', 'how', 'reasonable', 'risk', 'destroyer', 'frigate', 'fight', 'terrorist', 'boat', 'pirate', 'capable', 'warship', 'smaller', 'combatant', 'many', 'surface', 'vessel', 'perform', 'maritime', 'security', 'operation', 'nato', 'task', 'force', 'example', 'sophisticated', 'warship', 'capable', 'warfare', 'asuw', 'warfare', 'aaw', 'warfare', 'asw', 'sophisticated', 'capable', 'fleet', 'able', 'achieve', 'good', 'result', 'expeditionary', 'warfare', 'against', 'strong', 'enemy', 'capability', 'ship', 'will', 'probably', 'used', 'le', 'total', 'lifetime', 'seems', 'sound', 'reason', 'build', 'capable', 'ship', 'case', 'conventional', 'war', 'claim', 'capable', 'ship', 'built', 'used', 'small', 'period', 'lifetime', 'nevertheless', 'navy', 'should', 'optimize', 'effort', 'resource', 'some', 'way', 'find', 'better', 'mix', 'vessel', 'type', 'system', 'constitute', 'vessel']\n"
     ]
    }
   ],
   "source": [
    "# Import WordNetLemmatizer\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Topic Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Bag-of-Words\n",
    "\n",
    "Bag-of-words model can be used to represent the text as the bag of its words. It is mainly used as a tool of feature generation. One of the main properties of the bag-of-words model is that it completely ignores the order of the tokens in the document that is encoded, which is where the name bag-of-words comes from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag-of-words Using `Counter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 16), ('in', 13), ('the', 9), ('of', 9), ('.', 8), ('a', 7), ('to', 7), ('and', 6), ('that', 6), ('are', 5)]\n"
     ]
    }
   ],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "# Tokenize the article\n",
    "tokens = word_tokenize(my_string_long)\n",
    "\n",
    "# Convert the tokens into lowercase\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "# Create a Counter with the lowercase tokens of the raw text\n",
    "bow_simple = Counter(lower_tokens)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow_simple.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('operation', 5), ('capable', 5), ('maritime', 4), ('vessel', 4), ('used', 4), ('ship', 4), ('warfare', 4), ('mission', 3), ('warship', 3), ('naval', 2)]\n"
     ]
    }
   ],
   "source": [
    "# Create the bag-of-words of the lemmatized text\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag-of-words Using `gensim`\n",
    "\n",
    "Word vectors are multi-dimensional mathematical representations of words created using _deep learning_ methods. They give us insight into relationships between words in a corpus. We can create and query a corpus with `gensim` module.\n",
    "\n",
    "Gensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora.\n",
    "\n",
    "Check [gensim](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/gensim%20Quick%20Start.ipynb) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = \"raw-corpus/\"\n",
    "files = os.listdir(path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# Import text files to generate raw corpus\n",
    "for file in files:\n",
    "    if file.endswith(\".txt\"):\n",
    "        with open(path + file, 'r') as f:\n",
    "            raw_corpus.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing text (simple)\n",
    "\n",
    "# Lowercase each document, split it by white space and filter out stopwords\n",
    "texts = [[word for word in document.lower().split() if word not in english_stops]\n",
    "         for document in raw_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding, we want to associate each word in the corpus with a *unique integer ID*. We can do this using the `gensim.corpora.Dictionary` class. This dictionary defines the vocabulary of all words that our processing knows about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(218 unique tokens: ['analysis', 'cleansing,', 'conclusions', 'data', 'decision-making.']...)\n"
     ]
    }
   ],
   "source": [
    "# Import Dictionary\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Create a Dictionary from the articles\n",
    "dictionary = Dictionary(texts)\n",
    "\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our processed corpus has n=218 unique words in it, which means that each document will be represented by a n-dimensional vector under the bag-of-words model. We can use the dictionary to turn tokenized documents into these n-dimensional vectors. We can see what these IDs correspond to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'analysis': 0, 'cleansing,': 1, 'conclusions': 2, 'data': 3, 'decision-making.': 4, 'discovering': 5, 'goal': 6, 'information,': 7, 'informing': 8, 'inspecting,': 9, 'modeling': 10, 'process': 11, 'supporting': 12, 'transforming': 13, 'useful': 14, '0': 15, '1': 16, '1,': 17, 'between': 18, 'certainty.': 19, 'events': 20, 'glossary': 21, 'impossibility': 22, 'indicates': 23, 'likelihood': 24, 'measure': 25, 'number': 26, 'occur.': 27, 'probability': 28, 'quantifies': 29, 'quantifying': 30, 'see': 31, 'sparsly': 32, 'speaking,': 33, 'statistics.': 34, 'where,': 35, 'will': 36, '\"a': 37, '\"can': 38, '\"computing': 39, '\"training': 40, '(as': 41, '1959': 42, '====': 43, 'across': 44, 'alan': 45, 'algorithm': 46, 'algorithms': 47, 'also': 48, 'analytics.': 49, 'application': 50, 'applications,': 51, 'arthur': 52, 'artificial': 53, 'at': 54, 'based': 55, 'being': 56, 'build': 57, 'business': 58, 'by': 59, 'characteristics': 60, 'class': 61, 'closely': 62, 'cognitive': 63, 'coined': 64, 'computational': 65, 'computer': 66, 'computers.': 67, 'concerned': 68, 'constructing': 69, 'could': 70, 'data\",': 71, 'data,': 72, 'decisions': 73, 'defining': 74, 'definition': 75, 'delivers': 76, 'develop': 77, 'do': 78, 'do?\".': 79, 'domains': 80, 'e': 81, 'e.\"': 82, 'effectively': 83, 'email': 84, 'entities)': 85, 'experience': 86, 'explicit': 87, 'explicitly': 88, 'exploratory': 89, 'exposed.': 90, 'field': 91, 'field:': 92, 'filtering,': 93, 'focuses': 94, 'follows': 95, 'formal': 96, 'from': 97, 'fundamentally': 98, 'his': 99, 'if': 100, 'implications': 101, 'improves': 102, 'infeasible': 103, 'inference': 104, 'instead.': 105, 'instructions': 106, 'instructions,': 107, 'intelligence\",': 108, 'intelligence.': 109, 'its': 110, 'known': 111, 'learn': 112, 'learning': 113, 'learning,': 114, 'learning.': 115, 'm.': 116, 'machine': 117, 'machinery': 118, 'machines': 119, 'make': 120, 'making': 121, 'mathematical': 122, 'measured': 123, 'methods,': 124, 'mining': 125, 'mitchell': 126, 'model': 127, 'models': 128, 'more': 129, 'name': 130, 'offers': 131, 'operational': 132, 'optimization': 133, 'order': 134, 'overview': 135, 'p': 136, 'p,': 137, 'paper': 138, 'patterns': 139, 'perform': 140, 'performance': 141, 'performing': 142, 'possessed': 143, 'predictions': 144, 'predictive': 145, 'problems,': 146, 'program': 147, 'programmed': 148, 'proposal': 149, 'provided': 150, 'question': 151, 'quoted,': 152, 'rather': 153, 'referred': 154, 'related': 155, 'relying': 156, 'replaced': 157, 'respect': 158, 'said': 159, 'sample': 160, 'samuel.': 161, 'scientific': 162, 'seen': 163, 'some': 164, 'specific': 165, 'statistical': 166, 'statistics,': 167, 'studied': 168, 'study': 169, 'subset': 170, 'systems': 171, 't': 172, 't,': 173, 'task': 174, 'task.': 175, 'tasks': 176, 'terms.': 177, 'theory': 178, 'think?\"': 179, 'thinking': 180, 'through': 181, 'tom': 182, \"turing's\": 183, 'unsupervised': 184, 'use': 185, 'used': 186, 'using': 187, 'variety': 188, 'various': 189, 'vision,': 190, 'was': 191, 'we': 192, 'what': 193, 'where': 194, 'which': 195, 'wide': 196, 'widely': 197, 'within': 198, 'without': 199, 'analysis,': 200, 'applying': 201, 'begin': 202, 'branch': 203, 'collection,': 204, 'conventional': 205, 'industrial,': 206, 'interpretation': 207, 'mathematics': 208, 'organization,': 209, 'population': 210, 'presentation.': 211, 'problem,': 212, 'scientific,': 213, 'social': 214, 'statistics': 215, 'studied.': 216, 'working': 217}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 3\n",
      "word:  data\n"
     ]
    }
   ],
   "source": [
    "# Select the id for \"data\"\n",
    "data_id = dictionary.token2id.get(\"data\")\n",
    "\n",
    "print(\"id:\", data_id)\n",
    "\n",
    "# Use data_id with the dictionary to print the word\n",
    "print(\"word: \", dictionary.get(data_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create the bag-of-word representation for a document using the `doc2bow` method of the dictionary, which returns a sparse representation of the word counts. The first entry in each tuple corresponds to the ID of the token in the dictionary, the second corresponds to the count of this token. \n",
    "\n",
    "`Dictionary.doc2bow`: Convert `document` into the bag-of-words (BoW) format = list of `(token_id, token_count)` tuples.\n",
    "\n",
    "Now, we can convert our entire original corpus to a list of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 1), (127, 1), (166, 2), (200, 1), (201, 1), (202, 1), (203, 1), (204, 1), (205, 1), (206, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Create a MmCorpus (MatrixMarket format)\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Print the first 10 word ids with their frequency counts from the fourth document\n",
    "print(bow_corpus[3][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of documents (texts)\n",
    "len(bow_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even vectorize a phrase, \"Data is everywhere\", which was not in our original corpus. Note that the word \"everywhere\" did not occur in the original corpus and so it was not included in the vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 1)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text = \"Data is everywhere\"\n",
    "\n",
    "# Create the bag-of-word representation for the new_text using the doc2bow method of the dictionary\n",
    "new_vec = dictionary.doc2bow(new_text.lower().split())\n",
    "\n",
    "# Print the new_vec\n",
    "new_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(166, 2), (215, 2), (3, 1), (127, 1), (200, 1), (201, 1), (202, 1), (203, 1), (204, 1), (205, 1), (206, 1), (207, 1), (208, 1), (209, 1), (210, 1), (211, 1), (212, 1), (213, 1), (214, 1), (216, 1), (217, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Save the fourth document\n",
    "doc = bow_corpus[3]\n",
    "\n",
    "# Sort the doc for frequency\n",
    "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "print(bow_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistical 2\n",
      "statistics 2\n",
      "data 1\n",
      "model 1\n",
      "analysis, 1\n"
     ]
    }
   ],
   "source": [
    "# Print the top 5 words of the document alongside the count\n",
    "for word_id, word_count in bow_doc[:5]:\n",
    "    print(dictionary.get(word_id), word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine 11\n",
      "learning 8\n",
      "data 5\n",
      "algorithms 4\n",
      "probability 3\n"
     ]
    }
   ],
   "source": [
    "# Create the defaultdict\n",
    "total_word_count = defaultdict(int)\n",
    "for word_id, word_count in chain.from_iterable(bow_corpus): #itertools.chain\n",
    "    total_word_count[word_id] += word_count\n",
    "\n",
    "# Create a sorted list from the defaultdict\n",
    "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) \n",
    "\n",
    "# Print the top 5 words across all documents alongside the count\n",
    "for word_id, word_count in sorted_word_count[:5]:\n",
    "    print(dictionary.get(word_id), word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idf\n",
    "\n",
    "Vectorized corpus can be transformed using models. The tf-idf model transforms vectors from the bag-of-words representation to a vector space where the frequency counts are weighted according to the relative rarity of each word in the corpus.\n",
    "\n",
    "> In a large text corpus, some words will be very present (e.g. “the”, “a”, “is” in English) hence carrying very little meaningful information about the actual contents of the document. If we were to feed the direct count data directly to a classifier those very frequent terms would shadow the frequencies of rarer yet more interesting terms.\n",
    "In order to re-weight the count features into floating point values suitable for usage by a classifier it is very common to use the tf–idf transform.[ref](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n",
    "\n",
    "We can now determine new significant terms for the corpus by applying gensim's tf-idf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 0.043951374386869975), (127, 0.10589735737912483), (166, 0.21179471475824965), (200, 0.21179471475824965), (201, 0.21179471475824965)]\n"
     ]
    }
   ],
   "source": [
    "# Import TfidfModel\n",
    "# from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "# Create a new TfidfModel using the bow_corpus (Train the model)\n",
    "tfidf = TfidfModel(bow_corpus)\n",
    "\n",
    "# Calculate the tfidf weights of doc\n",
    "tfidf_weights = tfidf[doc]\n",
    "\n",
    "# Print the first five weights\n",
    "print(tfidf_weights[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics 0.4235894295164993\n",
      "statistical 0.21179471475824965\n",
      "analysis, 0.21179471475824965\n",
      "applying 0.21179471475824965\n",
      "begin 0.21179471475824965\n"
     ]
    }
   ],
   "source": [
    "# Sort the weights from highest to lowest\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 weighted words\n",
    "for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "    print(dictionary.get(term_id), weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tfidf model returns a list of tuples, where the first entry is the token ID and the second entry is the tf-idf weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.33018847715162497),\n",
       " (3, 0.13704059984770006),\n",
       " (113, 0.6603769543032499),\n",
       " (117, 0.6603769543032499)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the \"Data analysis and machine learning\" string\n",
    "tfidf[dictionary.doc2bow(\"Data analysis and machine learning\".lower().split())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Named Entity Recognition (NER)\n",
    "\n",
    "Named entity recognition is an information extraction process that seeks to locate and classify named entities in text into pre-defined categories answering the questions who, what, where, when etc. These categories include names, organizations, locations, quantities, percentages, etc. NER can help answering  real-world questions, such as:\n",
    "- Which politicians were mentioned in the news article?\n",
    "- Were specified products mentioned in complaints or reviews?\n",
    "- Does the tweet contain the name of an organization or a person? Does the tweet also contains location information?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences:\n",
      " ['Data science (From Wikipedia)\\n\\nData science is a multi-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from structured and unstructured data.', 'Data science is the same concept as data mining and big data: \"use the most powerful hardware, the most powerful programming systems, and the most efficient algorithms to solve problems\".', 'Data science is a \"concept to unify statistics, data analysis, machine learning and their related methods\" in order to \"understand and analyze actual phenomena\" with data.', 'It employs techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, and information science.', 'Turing award winner Jim Gray imagined data science as a \"fourth paradigm\" of science (empirical, theoretical, computational and now data-driven) and asserted that \"everything about science is changing because of the impact of information technology\" and the data deluge.', 'In 2015, the American Statistical Association identified database management, statistics and machine learning, and distributed and parallel systems as the three emerging foundational professional communities\\n\\nIn 2012, when Harvard Business Review called it \"The Sexiest Job of the 21st Century\", the term \"data science\" became a buzzword.', 'It is now often used interchangeably with earlier concepts like business analytics, business intelligence, predictive modeling, and statistics.', 'Even the suggestion that data science is sexy was paraphrasing Hans Rosling, featured in a 2011 BBC documentary with the quote, \"Statistics is now the sexiest subject around.\"', 'Nate Silver referred to data science as a sexed up term for statistics.', 'In many cases, earlier approaches and solutions are now simply rebranded as \"data science\" to be more attractive, which can cause the term to become \"diluted beyond usefulness.\"', 'While many university programs now offer a data science degree, there exists no consensus on a definition or suitable curriculum contents.', 'To its discredit, however, many data-science and big-data projects fail to deliver useful results, often as a result of poor management and utilization of resources.']\n",
      "\n",
      "Tokens:\n",
      " [['Data', 'science', '(', 'From', 'Wikipedia', ')', 'Data', 'science', 'is', 'a', 'multi-disciplinary', 'field', 'that', 'uses', 'scientific', 'methods', ',', 'processes', ',', 'algorithms', 'and', 'systems', 'to', 'extract', 'knowledge', 'and', 'insights', 'from', 'structured', 'and', 'unstructured', 'data', '.'], ['Data', 'science', 'is', 'the', 'same', 'concept', 'as', 'data', 'mining', 'and', 'big', 'data', ':', '``', 'use', 'the', 'most', 'powerful', 'hardware', ',', 'the', 'most', 'powerful', 'programming', 'systems', ',', 'and', 'the', 'most', 'efficient', 'algorithms', 'to', 'solve', 'problems', \"''\", '.'], ['Data', 'science', 'is', 'a', '``', 'concept', 'to', 'unify', 'statistics', ',', 'data', 'analysis', ',', 'machine', 'learning', 'and', 'their', 'related', 'methods', \"''\", 'in', 'order', 'to', '``', 'understand', 'and', 'analyze', 'actual', 'phenomena', \"''\", 'with', 'data', '.'], ['It', 'employs', 'techniques', 'and', 'theories', 'drawn', 'from', 'many', 'fields', 'within', 'the', 'context', 'of', 'mathematics', ',', 'statistics', ',', 'computer', 'science', ',', 'and', 'information', 'science', '.'], ['Turing', 'award', 'winner', 'Jim', 'Gray', 'imagined', 'data', 'science', 'as', 'a', '``', 'fourth', 'paradigm', \"''\", 'of', 'science', '(', 'empirical', ',', 'theoretical', ',', 'computational', 'and', 'now', 'data-driven', ')', 'and', 'asserted', 'that', '``', 'everything', 'about', 'science', 'is', 'changing', 'because', 'of', 'the', 'impact', 'of', 'information', 'technology', \"''\", 'and', 'the', 'data', 'deluge', '.'], ['In', '2015', ',', 'the', 'American', 'Statistical', 'Association', 'identified', 'database', 'management', ',', 'statistics', 'and', 'machine', 'learning', ',', 'and', 'distributed', 'and', 'parallel', 'systems', 'as', 'the', 'three', 'emerging', 'foundational', 'professional', 'communities', 'In', '2012', ',', 'when', 'Harvard', 'Business', 'Review', 'called', 'it', '``', 'The', 'Sexiest', 'Job', 'of', 'the', '21st', 'Century', \"''\", ',', 'the', 'term', '``', 'data', 'science', \"''\", 'became', 'a', 'buzzword', '.'], ['It', 'is', 'now', 'often', 'used', 'interchangeably', 'with', 'earlier', 'concepts', 'like', 'business', 'analytics', ',', 'business', 'intelligence', ',', 'predictive', 'modeling', ',', 'and', 'statistics', '.'], ['Even', 'the', 'suggestion', 'that', 'data', 'science', 'is', 'sexy', 'was', 'paraphrasing', 'Hans', 'Rosling', ',', 'featured', 'in', 'a', '2011', 'BBC', 'documentary', 'with', 'the', 'quote', ',', '``', 'Statistics', 'is', 'now', 'the', 'sexiest', 'subject', 'around', '.', \"''\"], ['Nate', 'Silver', 'referred', 'to', 'data', 'science', 'as', 'a', 'sexed', 'up', 'term', 'for', 'statistics', '.'], ['In', 'many', 'cases', ',', 'earlier', 'approaches', 'and', 'solutions', 'are', 'now', 'simply', 'rebranded', 'as', '``', 'data', 'science', \"''\", 'to', 'be', 'more', 'attractive', ',', 'which', 'can', 'cause', 'the', 'term', 'to', 'become', '``', 'diluted', 'beyond', 'usefulness', '.', \"''\"], ['While', 'many', 'university', 'programs', 'now', 'offer', 'a', 'data', 'science', 'degree', ',', 'there', 'exists', 'no', 'consensus', 'on', 'a', 'definition', 'or', 'suitable', 'curriculum', 'contents', '.'], ['To', 'its', 'discredit', ',', 'however', ',', 'many', 'data-science', 'and', 'big-data', 'projects', 'fail', 'to', 'deliver', 'useful', 'results', ',', 'often', 'as', 'a', 'result', 'of', 'poor', 'management', 'and', 'utilization', 'of', 'resources', '.']]\n"
     ]
    }
   ],
   "source": [
    "with open(\"articles/article-data-science.txt\", 'r') as file:\n",
    "    article = file.read()\n",
    "\n",
    "# Tokenize the article into sentences\n",
    "sentences = nltk.sent_tokenize(article)\n",
    "\n",
    "print(\"Sentences:\\n\", sentences)\n",
    "\n",
    "# Tokenize each sentence into words\n",
    "token_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "print(\"\\nTokens:\\n\", token_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parts of Speech:\n",
      " [[('Data', 'NNS'), ('science', 'NN'), ('(', '('), ('From', 'IN'), ('Wikipedia', 'NNP'), (')', ')'), ('Data', 'NNP'), ('science', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('multi-disciplinary', 'JJ'), ('field', 'NN'), ('that', 'WDT'), ('uses', 'VBZ'), ('scientific', 'JJ'), ('methods', 'NNS'), (',', ','), ('processes', 'NNS'), (',', ','), ('algorithms', 'NN'), ('and', 'CC'), ('systems', 'NNS'), ('to', 'TO'), ('extract', 'VB'), ('knowledge', 'NN'), ('and', 'CC'), ('insights', 'NNS'), ('from', 'IN'), ('structured', 'VBN'), ('and', 'CC'), ('unstructured', 'JJ'), ('data', 'NNS'), ('.', '.')], [('Data', 'NNP'), ('science', 'NN'), ('is', 'VBZ'), ('the', 'DT'), ('same', 'JJ'), ('concept', 'NN'), ('as', 'IN'), ('data', 'NNS'), ('mining', 'NN'), ('and', 'CC'), ('big', 'JJ'), ('data', 'NNS'), (':', ':'), ('``', '``'), ('use', 'IN'), ('the', 'DT'), ('most', 'RBS'), ('powerful', 'JJ'), ('hardware', 'NN'), (',', ','), ('the', 'DT'), ('most', 'RBS'), ('powerful', 'JJ'), ('programming', 'VBG'), ('systems', 'NNS'), (',', ','), ('and', 'CC'), ('the', 'DT'), ('most', 'RBS'), ('efficient', 'JJ'), ('algorithms', 'NN'), ('to', 'TO'), ('solve', 'VB'), ('problems', 'NNS'), (\"''\", \"''\"), ('.', '.')], [('Data', 'NNP'), ('science', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('``', '``'), ('concept', 'NN'), ('to', 'TO'), ('unify', 'VB'), ('statistics', 'NNS'), (',', ','), ('data', 'NNS'), ('analysis', 'NN'), (',', ','), ('machine', 'NN'), ('learning', 'NN'), ('and', 'CC'), ('their', 'PRP$'), ('related', 'JJ'), ('methods', 'NNS'), (\"''\", \"''\"), ('in', 'IN'), ('order', 'NN'), ('to', 'TO'), ('``', '``'), ('understand', 'VB'), ('and', 'CC'), ('analyze', 'VB'), ('actual', 'JJ'), ('phenomena', 'NNS'), (\"''\", \"''\"), ('with', 'IN'), ('data', 'NNS'), ('.', '.')], [('It', 'PRP'), ('employs', 'VBZ'), ('techniques', 'NNS'), ('and', 'CC'), ('theories', 'NNS'), ('drawn', 'VBP'), ('from', 'IN'), ('many', 'JJ'), ('fields', 'NNS'), ('within', 'IN'), ('the', 'DT'), ('context', 'NN'), ('of', 'IN'), ('mathematics', 'NNS'), (',', ','), ('statistics', 'NNS'), (',', ','), ('computer', 'NN'), ('science', 'NN'), (',', ','), ('and', 'CC'), ('information', 'NN'), ('science', 'NN'), ('.', '.')], [('Turing', 'VBG'), ('award', 'NN'), ('winner', 'NN'), ('Jim', 'NNP'), ('Gray', 'NNP'), ('imagined', 'VBD'), ('data', 'NNS'), ('science', 'NN'), ('as', 'IN'), ('a', 'DT'), ('``', '``'), ('fourth', 'JJ'), ('paradigm', 'NN'), (\"''\", \"''\"), ('of', 'IN'), ('science', 'NN'), ('(', '('), ('empirical', 'JJ'), (',', ','), ('theoretical', 'JJ'), (',', ','), ('computational', 'JJ'), ('and', 'CC'), ('now', 'RB'), ('data-driven', 'JJ'), (')', ')'), ('and', 'CC'), ('asserted', 'VBD'), ('that', 'IN'), ('``', '``'), ('everything', 'NN'), ('about', 'IN'), ('science', 'NN'), ('is', 'VBZ'), ('changing', 'VBG'), ('because', 'IN'), ('of', 'IN'), ('the', 'DT'), ('impact', 'NN'), ('of', 'IN'), ('information', 'NN'), ('technology', 'NN'), (\"''\", \"''\"), ('and', 'CC'), ('the', 'DT'), ('data', 'NNS'), ('deluge', 'NN'), ('.', '.')], [('In', 'IN'), ('2015', 'CD'), (',', ','), ('the', 'DT'), ('American', 'NNP'), ('Statistical', 'NNP'), ('Association', 'NNP'), ('identified', 'VBD'), ('database', 'NN'), ('management', 'NN'), (',', ','), ('statistics', 'NNS'), ('and', 'CC'), ('machine', 'NN'), ('learning', 'NN'), (',', ','), ('and', 'CC'), ('distributed', 'VBD'), ('and', 'CC'), ('parallel', 'JJ'), ('systems', 'NNS'), ('as', 'IN'), ('the', 'DT'), ('three', 'CD'), ('emerging', 'VBG'), ('foundational', 'JJ'), ('professional', 'JJ'), ('communities', 'NNS'), ('In', 'IN'), ('2012', 'CD'), (',', ','), ('when', 'WRB'), ('Harvard', 'NNP'), ('Business', 'NNP'), ('Review', 'NNP'), ('called', 'VBD'), ('it', 'PRP'), ('``', '``'), ('The', 'DT'), ('Sexiest', 'NNP'), ('Job', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('21st', 'JJ'), ('Century', 'NNP'), (\"''\", \"''\"), (',', ','), ('the', 'DT'), ('term', 'NN'), ('``', '``'), ('data', 'NNS'), ('science', 'NN'), (\"''\", \"''\"), ('became', 'VBD'), ('a', 'DT'), ('buzzword', 'NN'), ('.', '.')], [('It', 'PRP'), ('is', 'VBZ'), ('now', 'RB'), ('often', 'RB'), ('used', 'VBN'), ('interchangeably', 'RB'), ('with', 'IN'), ('earlier', 'JJR'), ('concepts', 'NNS'), ('like', 'IN'), ('business', 'NN'), ('analytics', 'NNS'), (',', ','), ('business', 'NN'), ('intelligence', 'NN'), (',', ','), ('predictive', 'JJ'), ('modeling', 'NN'), (',', ','), ('and', 'CC'), ('statistics', 'NNS'), ('.', '.')], [('Even', 'RB'), ('the', 'DT'), ('suggestion', 'NN'), ('that', 'IN'), ('data', 'NNS'), ('science', 'NN'), ('is', 'VBZ'), ('sexy', 'VBN'), ('was', 'VBD'), ('paraphrasing', 'VBG'), ('Hans', 'NNPS'), ('Rosling', 'NNP'), (',', ','), ('featured', 'VBD'), ('in', 'IN'), ('a', 'DT'), ('2011', 'CD'), ('BBC', 'NNP'), ('documentary', 'NN'), ('with', 'IN'), ('the', 'DT'), ('quote', 'NN'), (',', ','), ('``', '``'), ('Statistics', 'NNPS'), ('is', 'VBZ'), ('now', 'RB'), ('the', 'DT'), ('sexiest', 'JJS'), ('subject', 'JJ'), ('around', 'IN'), ('.', '.'), (\"''\", \"''\")], [('Nate', 'NNP'), ('Silver', 'NNP'), ('referred', 'VBD'), ('to', 'TO'), ('data', 'VB'), ('science', 'NN'), ('as', 'IN'), ('a', 'DT'), ('sexed', 'JJ'), ('up', 'RP'), ('term', 'NN'), ('for', 'IN'), ('statistics', 'NNS'), ('.', '.')], [('In', 'IN'), ('many', 'JJ'), ('cases', 'NNS'), (',', ','), ('earlier', 'JJR'), ('approaches', 'NNS'), ('and', 'CC'), ('solutions', 'NNS'), ('are', 'VBP'), ('now', 'RB'), ('simply', 'RB'), ('rebranded', 'VBD'), ('as', 'IN'), ('``', '``'), ('data', 'NNS'), ('science', 'NN'), (\"''\", \"''\"), ('to', 'TO'), ('be', 'VB'), ('more', 'RBR'), ('attractive', 'JJ'), (',', ','), ('which', 'WDT'), ('can', 'MD'), ('cause', 'VB'), ('the', 'DT'), ('term', 'NN'), ('to', 'TO'), ('become', 'VB'), ('``', '``'), ('diluted', 'JJ'), ('beyond', 'IN'), ('usefulness', 'JJ'), ('.', '.'), (\"''\", \"''\")], [('While', 'IN'), ('many', 'JJ'), ('university', 'NN'), ('programs', 'NNS'), ('now', 'RB'), ('offer', 'VBP'), ('a', 'DT'), ('data', 'NN'), ('science', 'NN'), ('degree', 'NN'), (',', ','), ('there', 'EX'), ('exists', 'VBZ'), ('no', 'DT'), ('consensus', 'NN'), ('on', 'IN'), ('a', 'DT'), ('definition', 'NN'), ('or', 'CC'), ('suitable', 'JJ'), ('curriculum', 'NN'), ('contents', 'NNS'), ('.', '.')], [('To', 'TO'), ('its', 'PRP$'), ('discredit', 'NN'), (',', ','), ('however', 'RB'), (',', ','), ('many', 'JJ'), ('data-science', 'NN'), ('and', 'CC'), ('big-data', 'JJ'), ('projects', 'NNS'), ('fail', 'VBP'), ('to', 'TO'), ('deliver', 'VB'), ('useful', 'JJ'), ('results', 'NNS'), (',', ','), ('often', 'RB'), ('as', 'IN'), ('a', 'DT'), ('result', 'NN'), ('of', 'IN'), ('poor', 'JJ'), ('management', 'NN'), ('and', 'CC'), ('utilization', 'NN'), ('of', 'IN'), ('resources', 'NNS'), ('.', '.')]]\n",
      "\n",
      "Named Entity Chunks:\n",
      " <generator object ParserI.parse_sents.<locals>.<genexpr> at 0x1a18544660>\n"
     ]
    }
   ],
   "source": [
    "# Tag each tokenized sentence into parts of speech\n",
    "pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n",
    "\n",
    "print(\"\\nParts of Speech:\\n\", pos_sentences)\n",
    "\n",
    "# Create the named entity chunks\n",
    "chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=True)\n",
    "\n",
    "print(\"\\nNamed Entity Chunks:\\n\", chunked_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NE Wikipedia/NNP)\n",
      "(NE Data/NNP)\n",
      "(NE Data/NNP)\n",
      "(NE Jim/NNP Gray/NNP)\n",
      "(NE American/NNP Statistical/NNP Association/NNP)\n",
      "(NE Harvard/NNP Business/NNP Review/NNP)\n",
      "(NE Sexiest/NNP Job/NNP)\n",
      "(NE Century/NNP)\n",
      "(NE Nate/NNP Silver/NNP)\n"
     ]
    }
   ],
   "source": [
    "# Test for stems of the tree with 'NE' tags\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        # Determine if each chunk has a 'label'\n",
    "        if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n",
    "            print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NE Artificial/JJ)\n",
      "(NE Tesler/NNP)\n",
      "(NE Modern/JJ)\n",
      "(NE Artificial/JJ)\n",
      "(NE Analytical/JJ AI/NNP)\n",
      "(NE Humanized/NNP AI/NNP)\n",
      "(NE Artificial/JJ)\n",
      "(NE AI/NNP)\n",
      "(NE AI/NNP)\n",
      "(NE Approaches/NNP)\n",
      "(NE AI/NNP)\n",
      "(NE AI/NNP)\n",
      "(NE History/NNP)\n",
      "(NE Mary/NNP Shelley/NNP)\n",
      "(NE Karel/NNP Čapek/NNP)\n",
      "(NE Rossum/NNP)\n",
      "(NE Universal/NNP Robots/NNP)\n",
      "(NE Alan/NNP Turing/NNP)\n",
      "(NE AI/NNP)\n",
      "(NE McCullouch/NNP)\n",
      "(NE Pitts/NNP)\n",
      "(NE AI/NNP)\n",
      "(NE Dartmouth/NNP College/NNP)\n",
      "(NE CMU/NNP)\n",
      "(NE Herbert/NNP Simon/NNP)\n",
      "(NE CMU/NNP)\n",
      "(NE John/NNP McCarthy/NNP)\n",
      "(NE MIT/NNP)\n",
      "(NE Marvin/NNP Minsky/NNP)\n",
      "(NE MIT/NNP)\n",
      "(NE Arthur/NNP Samuel/NNP)\n",
      "(NE IBM/NNP)\n",
      "(NE AI/NNP)\n",
      "(NE Logic/NNP Theorist/NNP)\n",
      "(NE English/NNP)\n",
      "(NE U.S./NNP)\n",
      "(NE Defense/NNP)\n",
      "(NE AI/NNP)\n",
      "(NE Herbert/NNP Simon/NNP)\n",
      "(NE Marvin/NNP Minsky/NNP)\n",
      "(NE Progress/NN)\n",
      "(NE Sir/NNP James/NNP Lighthill/NNP)\n",
      "(NE US/NNP Congress/NNP)\n",
      "(NE U.S./NNP)\n",
      "(NE British/JJ)\n",
      "(NE AI/NNP)\n",
      "(NE AI/NNP)\n",
      "(NE Japan/NNP)\n",
      "(NE British/JJ)\n",
      "(NE Lisp/NNP Machine/NNP)\n",
      "(NE AI/NNP)\n",
      "(NE AI/NNP)\n",
      "(NE Moore/NNP)\n",
      "(NE Deep/JJ Blue/NNP)\n",
      "(NE Garry/NNP Kasparov/NNP)\n",
      "(NE IBM/NNP)\n",
      "(NE Watson/NNP)\n",
      "(NE Brad/NNP Rutter/NNP)\n",
      "(NE Ken/NNP Jennings/NNP)\n",
      "(NE Faster/NNP)\n",
      "(NE Kinect/NNP)\n",
      "(NE Xbox/NNP One/NNP)\n",
      "(NE AlphaGo/NNP)\n",
      "(NE Lee/NNP Sedol/NNP)\n",
      "(NE AlphaGo/NNP)\n",
      "(NE Artificial/JJ Intelligence/NNP)\n",
      "(NE Chess/NNP)\n",
      "(NE Bloomberg/NNP)\n",
      "(NE Jack/NNP Clark/NNP)\n",
      "(NE AI/NNP Google/NNP)\n",
      "(NE Clark/NNP)\n",
      "(NE Microsoft/NNP)\n",
      "(NE Facebook/NNP)\n",
      "(NE China/NNP)\n"
     ]
    }
   ],
   "source": [
    "with open(\"articles/article-AI.txt\", 'r') as file:\n",
    "    article = file.read()\n",
    "\n",
    "# Tokenize the article into sentences\n",
    "sentences = nltk.sent_tokenize(article)\n",
    "\n",
    "# Tokenize each sentence into words\n",
    "token_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Tag each tokenized sentence into parts of speech\n",
    "pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n",
    "\n",
    "# Create the named entity chunks\n",
    "chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=True)\n",
    "\n",
    "# Test for stems of the tree with 'NE' tags\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        # Determine if each chunk has a 'label'\n",
    "        if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n",
    "            print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from collections import defaultdict\n",
    "\n",
    "# Create the defaultdict\n",
    "ner_categories = defaultdict(int)\n",
    "\n",
    "# Create the named entity chunks\n",
    "chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=False)\n",
    "\n",
    "# Create the nested for loop\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            ner_categories[chunk.label()] += 1\n",
    "            \n",
    "# Create a list from the dictionary keys for the chart labels\n",
    "labels = list(ner_categories.keys())\n",
    "\n",
    "# Create a list of the values\n",
    "values = [ner_categories.get(l) for l in labels]\n",
    "\n",
    "# Create the pie chart\n",
    "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER with SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy is an open-source software library for advanced Natural Language Processing, written in the programming languages Python and Cython. The library is published under the MIT license and currently offers statistical neural network models for English, German, Spanish, Portuguese, French, Italian, Dutch and multi-language NER, as well as tokenization for various other languages. \n",
    "\n",
    "Unlike NLTK, which is widely used for teaching and research, spaCy focuses on providing software for production usage. spaCy also supports deep learning workflows that allow connecting statistical models trained by popular machine learning libraries like TensorFlow, Keras, Scikit-learn or PyTorch. [source](https://en.wikipedia.org/wiki/SpaCy)\n",
    "\n",
    "NLP library similar to `gensim`, with different implementations. It focuses on creating NLP pipelines to generate models and corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`$ conda install -c conda-forge spacy`\n",
    "\n",
    "`$ python -m spacy download en`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORG AI\n",
      "ORG AI\n",
      "WORK_OF_ART Tesler's Theorem\n",
      "PERSON Artificial\n",
      "CARDINAL three\n",
      "DATE 1956\n",
      "DATE the years\n",
      "ORG AI\n",
      "ORG AI\n",
      "ORG AI\n",
      "DATE the twenty-first century\n",
      "PERSON Mary Shelley's\n",
      "PERSON Frankenstein\n",
      "PERSON Karel Čapek's\n",
      "PERSON R.U.R.\n",
      "PERSON Rossum\n",
      "ORG Universal Robots\n",
      "PERSON Alan Turing's\n",
      "CARDINAL 1\n",
      "ORDINAL first\n",
      "PERSON McCullouch\n",
      "PERSON Pitts\n",
      "DATE 1943\n",
      "ORG Dartmouth College\n",
      "DATE 1956\n",
      "PERSON Attendees Allen Newell\n",
      "ORG CMU\n",
      "PERSON Herbert Simon\n",
      "ORG CMU\n",
      "PERSON John McCarthy\n",
      "ORG MIT\n",
      "PERSON Marvin Minsky\n",
      "ORG MIT\n",
      "PERSON Arthur Samuel\n",
      "ORG IBM\n",
      "DATE 1954\n",
      "DATE 1959\n",
      "ORG Logic Theorist\n",
      "ORDINAL first\n",
      "DATE 1956\n",
      "LANGUAGE English\n",
      "DATE the middle of the 1960s\n",
      "GPE U.S.\n",
      "ORG the Department of Defense\n",
      "PERSON Herbert Simon\n",
      "DATE twenty years\n",
      "PERSON Marvin Minsky\n",
      "DATE 1974\n",
      "PERSON James Lighthill\n",
      "GPE US\n",
      "ORG Congress\n",
      "GPE U.S.\n",
      "NORP British\n",
      "ORG AI\n",
      "DATE The next few years\n",
      "DATE the early 1980s\n",
      "DATE 1985\n",
      "ORG AI\n",
      "MONEY over a billion dollars\n",
      "GPE Japan\n",
      "ORDINAL fifth\n",
      "GPE U.S\n",
      "NORP British\n",
      "DATE 1987\n",
      "ORDINAL second\n",
      "DATE the late 1990s\n",
      "DATE early 21st century\n",
      "PERSON Moore\n",
      "ORG Deep Blue\n",
      "ORDINAL first\n",
      "PERSON Garry Kasparov\n",
      "DATE 11 May 1997\n",
      "DATE 2011\n",
      "ORG IBM\n",
      "PERSON Watson\n",
      "CARDINAL two\n",
      "PERSON Brad Rutter\n",
      "PERSON Ken Jennings\n",
      "DATE 2012\n",
      "PERSON Kinect\n",
      "PRODUCT the Xbox 360\n",
      "PRODUCT the Xbox One\n",
      "DATE March 2016\n",
      "ORG AlphaGo\n",
      "CARDINAL 4\n",
      "CARDINAL 5\n",
      "PERSON Lee Sedol\n",
      "ORDINAL first\n",
      "PERSON AlphaGo\n",
      "CARDINAL three\n",
      "PERSON Ke Jie\n",
      "CARDINAL 1\n",
      "DATE two years\n",
      "ORG Artificial Intelligence\n",
      "ORG Chess\n",
      "PERSON Bloomberg\n",
      "PERSON Jack Clark\n",
      "DATE 2015\n",
      "DATE a landmark year\n",
      "ORG Google\n",
      "DATE 2012\n",
      "CARDINAL more than 2,700\n",
      "PERSON Clark\n",
      "DATE 2012\n",
      "ORG Microsoft\n",
      "ORG Skype\n",
      "PERSON Facebook\n",
      "DATE 2017\n",
      "CARDINAL one\n",
      "CARDINAL five\n",
      "DATE 2016\n",
      "GPE China\n"
     ]
    }
   ],
   "source": [
    "# import spacy\n",
    "\n",
    "# Instantiate the English model: nlp\n",
    "nlp = spacy.load('en', tagger=False, parser=False, matcher=False)\n",
    "\n",
    "# Create a new document: doc\n",
    "doc = nlp(article)\n",
    "\n",
    "# Print all of the found entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`spaCy` uses some extra NER Categories compared to `nltk` in its named-entity recognition. These include NORP, CARDINAL, MONEY, WORKOFART, LANGUAGE and EVENT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER with `polyglot` (Multilingual)\n",
    "\n",
    "`polyglot` is another NLP library which uses word vectors like `gensim` or `spaCy`. The main advantage of `polyglot` is that it uses vectors for more than 130 languages. \n",
    "> Polyglot is a natural language pipeline that supports massive multilingual applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polyglot.text import Text\n",
    "\n",
    "with open(\"articles/article-turkish.txt\", 'r') as file:\n",
    "    article = file.read()\n",
    "\n",
    "# Create a new text object using Polyglot's Text class: txt\n",
    "txt = Text(article)\n",
    "\n",
    "# Print each of the entities found\n",
    "for ent in txt.entities:\n",
    "    print(ent)\n",
    "    \n",
    "# Print the type of ent\n",
    "print(type(ent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of tuples: entities\n",
    "entities = [(ent.tag, ' '.join(ent)) for ent in txt.entities]\n",
    "\n",
    "# Print entities\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the count variable: count\n",
    "count = 0\n",
    "\n",
    "# Iterate over all the entities\n",
    "for ent in txt.entities:\n",
    "    # Check whether the entity contains 'Márquez' or 'Gabo'\n",
    "    if 'Márquez' in ent or \"Gabo\" in ent:\n",
    "        # Increment count\n",
    "        count += 1\n",
    "\n",
    "# Print count\n",
    "print(count)\n",
    "\n",
    "# Calculate the percentage of entities that refer to \"Gabo\": percentage\n",
    "percentage = count / len(txt.entities)\n",
    "print(percentage)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
